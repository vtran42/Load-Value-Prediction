{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Load Value Prediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Us8KMOAw3GnQ"
      },
      "source": [
        "# Final Project CS203 - LOAD VALUE PREDICTION\n",
        "\n",
        "---\n",
        "\n",
        "- Students: Shreya Singh, Vincent Tran. \n",
        "\n",
        "# Model\n",
        "Long Short-Term Memory, a type of Recurrent Neural Network. This Neural Network allows previous outputs to be used as inputs while having hidden states. \n",
        "\n",
        "# Why we use this\n",
        "RNNs can take one or more input vectors and produce one or more output vectors and the output(s) are influenced not just by weights applied on inputs like a regular NN, but also by a “hidden” state vector representing the context based on prior input(s)/output(s). In order to remedy the vanishing gradient problem of RNNs, LSTM model is used. LSTM (Long Short Term Memory) networks improve on the RNN model by introducing additional gates and a cell state, such that it fundamentally addresses the problem of keeping or resetting context. It is a solution to the vanishing gradient problem. RNN finds its application when we are dealing with sequential data or data with temporal relationships.\n",
        "\n",
        "# How we create the model\n",
        "\n",
        "- In our model, we split the data set by training set and test set. The PC address of load instruction memory, memory address, and value in the training set are used to build the model. (We only fed the Load values in the model) Then we feed the load instruction memory and memory address  into the model to get the predicted value. We compare the predicted value and actual value based on the confidence value(use Mean Squared Errors as the threshold) to decide whether or not we should choose the data. \n",
        "\n",
        "- If the square of the difference between actual and predicted value is less than the Mean Square Errors, we choose the predicted value, otherwise we don’t.\n",
        "\n",
        "- If the predicted value is a bit off, we update the model by changing the model parameters such as epochs and the size of training set, training window.\n",
        "If the prediction is close to actual, we keep our model. \n",
        "\n",
        "- Our prediction model based on the recent instances. \n",
        "\n",
        "# Model:\n",
        "- Input: \n",
        "  - The actual values at the memory address for training.\n",
        "  - Because the actual values are stored as long range hexadecimal, we convert the hexadecimal to integer.\n",
        "- Ouput: The predicted Value on the test data.\n",
        "\n",
        "# Limitation:\n",
        "- Because the training data is big, it would take a long time to build the model. \n",
        "- In our model, we are able to get the results with small data set (10000), and smaller number of epochs(60).\n",
        "\n",
        "# Feature work\n",
        "- Increasing number of epochs and using a larger dataset to improve the model. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VB1OlyDJ31L"
      },
      "source": [
        "# This is the coding for CS203 final project\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFPeRaKTDjMR"
      },
      "source": [
        "#!wget https://drive.google.com/drive/folders/1b1cqriOkf87Qb55WPyw04jrdM4Hths_X?usp=sharing\n",
        "#!wget https://drive.google.com/file/d/1xY1M8zelUpYNXI-wGgTEpU9ABKaYiBZX/view?usp=sharing\n",
        "#!wget https://drive.google.com/file/d/1RjFeKS2xJbGoOc1Id-DI-MhKnZw20flO/view?usp=sharing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMxUM7-oTECs"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkFwFv8MPZ35"
      },
      "source": [
        "# Upload the Data into the Colab folder\n",
        "\n",
        "---\n",
        "\n",
        "- The data must be raw data, and the format file must be .csv\n",
        "- the data should have three colums: Load Instruction PC, memory address, and value.\n",
        "- For example:  \n",
        "0x7f8545bfde1f:  R  0x7f8545c29e68,   0x000000000000000e"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "vz-jH8T_Uk2c",
        "outputId": "236c507a-5e3a-4fc6-bee4-33e04deddb25"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "     name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b39d4580-bfc3-4031-9fd8-9a17ce707f54\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b39d4580-bfc3-4031-9fd8-9a17ce707f54\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-6830d6a4c2b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muploaded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m   \u001b[0;32mwhile\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'complete'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     result = _output.eval_js(\n\u001b[1;32m     71\u001b[0m         'google.colab._files._uploadFilesContinue(\"{output_id}\")'.format(\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1xALW-1GhfJ"
      },
      "source": [
        "# Clean up the data\n",
        "\n",
        "---\n",
        "\n",
        "- Replace `:` by `,`\n",
        "- Remove 'R'\n",
        "- Note: In this project, we use the file name: pinatrace1.csv. If your file is different, please feel free to update. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "90H_hpbDFvmo"
      },
      "source": [
        "import fileinput\n",
        "import re\n",
        "filename = \"pinatrace1.csv\" # update your file name here\n",
        "\n",
        "with open(filename, 'r+') as f:\n",
        "    text = f.read()\n",
        "    text = re.sub(':',',',text)\n",
        "    text = re.sub('R','', text)\n",
        "    #text = text.replace('0x','')\n",
        "    f.seek(0)\n",
        "    f.write(text)\n",
        "    f.truncate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAF4NquvJcvT"
      },
      "source": [
        "# Add the Library\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OKacjbnsHH3d"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from ast import literal_eval "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-6f9f2fSI38"
      },
      "source": [
        "# Process the Data\n",
        "\n",
        "---\n",
        "\n",
        "- We convert the value from hexadecimal to decimal number for training model puporses. \n",
        "- We remove all the rows that contain #eof, NaN\n",
        "- We remove all duplicate rows.\n",
        "- Columns name:\n",
        "  - PCAddr: PC address of load instruction.\n",
        "  - memAddr: memory address of load instruction.\n",
        "  - value: the value of load instruction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DlFfL9E5Ml8v"
      },
      "source": [
        "data = pd.read_csv('pinatrace1.csv',names=['PCAddr','memAddr','value'])\n",
        "#data.head(5)\n",
        "data.sort_values(by=['PCAddr','memAddr'])\n",
        "print(data.size)\n",
        "#print(data.head(5))\n",
        "duplicate = data[data.duplicated(['PCAddr','memAddr','value'])]\n",
        "#print(duplicate,sep='\\n')\n",
        "print(\"duplicate: \", duplicate.size)\n",
        "#data.duplicated()\n",
        "data = data.drop_duplicates()\n",
        "print(\"after remove duplicate \" , data.size)\n",
        "#print(data.info())\n",
        "#type(data)\n",
        "data.drop(data.tail(1).index)\n",
        "data.dropna(subset=['memAddr'],inplace=True)\n",
        "#print(data.loc[[531885]])\n",
        "\n",
        "# Drop the last n rows in data\n",
        "#376123 - 351123 = 25000\n",
        "print(data.info())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBF3AYsgdlXd"
      },
      "source": [
        "# Reduce the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "950rnYBbVQmJ"
      },
      "source": [
        "# Input number of entries \n",
        "entries = input(\"Please enter number entries for data: \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hVYavRmCdodv"
      },
      "source": [
        "total = data.size/3;\n",
        "remove_n = int(total - (int)(entries))\n",
        "#df = pd.DataFrame({\"a\":[1,2,3,4], \"b\":[5,6,7,8]})\n",
        "drop_indices = np.random.choice(data.index, remove_n, replace=False)\n",
        "data = data.drop(drop_indices)\n",
        "\n",
        "totalRows = total - remove_n\n",
        "#n = 351123\n",
        "#data.drop(data.tail(n).index, \n",
        "#        inplace = True) \n",
        "print(data.info())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jb1sCcJwOQLH"
      },
      "source": [
        "#print(type(data['memAddr']))\n",
        "data['value'] = data['value'].str.strip()\n",
        "#print(type(s))\n",
        "s = data['value']\n",
        "counter = 0\n",
        "ls = list()\n",
        "#s.drop(s.tail(1))\n",
        "for i in s:\n",
        "  #print(type(i))\n",
        "  #test_string = i\n",
        "  #print(i)\n",
        "  #if(i is 'nan'):\n",
        "  #  break\n",
        "  i = literal_eval(str(i))\n",
        "  ls.append(i)\n",
        "  #print(\"Counter: \", counter)\n",
        "  #counter = counter + 1\n",
        "  #print(\"i: \", i)\n",
        "  #print(\"The decimal number of hexadecimal string : \" + str(literal_eval(str(i))))\n",
        "  #data['memAddr'].replace(i,res)\n",
        "  \n",
        "#print(data['memAddr'])\n",
        "\n",
        "# Convert address to integer, back and forth\n",
        "# Python3 code to demonstrate \n",
        "# converting hexadecimal string to decimal \n",
        "# Using ast.literal_eval() \n",
        "\n",
        "# initializing string  \n",
        "#test_string = '0x7f8545c29e68'\n",
        "#print(type(test_string))\n",
        "# printing original string  \n",
        "#print(\"The original string : \" + str(test_string)) \n",
        "  \n",
        "# using ast.literal_eval() \n",
        "# converting hexadecimal string to decimal \n",
        "#res = literal_eval(test_string) \n",
        "\n",
        "# print result \n",
        "#print(\"The decimal number of hexadecimal string : \" + str(res)) \n",
        "#print(hex(res))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oRFG6Gexfnfh"
      },
      "source": [
        "print(type(data))\n",
        "print(data.head(5))\n",
        "print(data.info())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "H-e42Pc-uxho"
      },
      "source": [
        "data.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kQ46X1Z-w_FF"
      },
      "source": [
        "print(data.size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8pvTA_WsxG8S"
      },
      "source": [
        "print(type(s))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "T8LfSNuqxjEu"
      },
      "source": [
        "data['valueInDec'] = pd.Series(ls, index=data.index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lon5Z-vG1Bxh"
      },
      "source": [
        "print(data.head(5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MLUOX6oq1HKx"
      },
      "source": [
        "# Convert the value in Decimal to hex\n",
        "decToAddr = list()\n",
        "for i in ls:\n",
        "  decToAddr.append(hex((i)))\n",
        "\n",
        "data['Value'] = pd.Series(decToAddr,index=data.index)\n",
        "print(data.head(5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ADu9pI_G2IXO"
      },
      "source": [
        "all_data = pd.DataFrame()\n",
        "all_data['PCAddr'] = data['PCAddr']\n",
        "all_data['memAddr']= data['memAddr']\n",
        "all_data['value'] = data['valueInDec']\n",
        "print(all_data.head(5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlYWk2HxTPV1"
      },
      "source": [
        "# Process the Reinforcement Learning- LSTM starts here\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrafBNngTb3M"
      },
      "source": [
        "## Check data type of each columns in the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vjMoSiuJ2xYH"
      },
      "source": [
        "all_data.columns\n",
        "print(all_data.info())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9-eyYMYTpTO"
      },
      "source": [
        "## Convert the value to decimal number"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aeX-r--z24gy"
      },
      "source": [
        "all_data = all_data['value'].values.astype(float) # convert object to float for value columns\n",
        "print(type(all_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LwyFmTx03G9T"
      },
      "source": [
        "print(all_data)\n",
        "print(len(all_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJdKr_MqTy-X"
      },
      "source": [
        "## Split the data into the test data and training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8_4ncsSuWZib"
      },
      "source": [
        "# Split data by percent\n",
        "splitRate = input(\"Please enter percent of data that will be test\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "C6idU_qb3qjH"
      },
      "source": [
        "test_data_size = (int)((float)(splitRate) * totalRows) # Total is 376123, after reducing data it is 25000 rows\n",
        "\n",
        "train_data = all_data[:-test_data_size]\n",
        "test_data = all_data[-test_data_size:]\n",
        "print(type(train_data))\n",
        "print(type(test_data))\n",
        "print(len(train_data))\n",
        "print(len(test_data))\n",
        "print(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8zYAn17T9YK"
      },
      "source": [
        "## Normalize the value columns\n",
        "\n",
        "---\n",
        "\n",
        "- the feature_range contains positive number only.\n",
        "- We will perform min/max scaling on the dataset which normalizes the data within a certain range of minimum and maximum values\n",
        "- [When the feature_range is symmetric (-1,1), we will get the negative number in ouput. Hence, the predicted value is negative which is impossible in the actual value. ]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bRdMN4Kp4yME"
      },
      "source": [
        "scaler = MinMaxScaler(feature_range=(0,1000)) # Need to check the range\n",
        "train_data_normalized = scaler.fit_transform(train_data.reshape(-1,1))\n",
        "print(train_data_normalized[:5])\n",
        "print(train_data_normalized[-5:0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7EUxg_BUH43"
      },
      "source": [
        "## Convert value into tensors because PyTorch models are trained using tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3rEnvohb6OQ4"
      },
      "source": [
        "train_data_normalized = torch.FloatTensor(train_data_normalized).view(-1)\n",
        "train_window = (int)((float)(splitRate) * totalRows)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8uCnwElUW_A"
      },
      "source": [
        "## Create the input sequence function\n",
        "\n",
        "- input: \n",
        "  - raw data\n",
        "  - train window\n",
        "- output: tuple\n",
        "  - first element is array contains where its length is train window.\n",
        "  - second element is first element in the next train_window.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4eVOMtsh6b73"
      },
      "source": [
        "def create_inout_sequences(input_data, tw):\n",
        "    inout_seq = []\n",
        "    L = len(input_data)\n",
        "    for i in range(L-tw):\n",
        "        train_seq = input_data[i:i+tw]\n",
        "        train_label = input_data[i+tw:i+tw+1]\n",
        "        inout_seq.append((train_seq ,train_label))\n",
        "    return inout_seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_v1RZzvO6fKc"
      },
      "source": [
        "train_inout_seq = create_inout_sequences(train_data_normalized, train_window)\n",
        "train_inout_seq[:4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKvGjUc47f9k"
      },
      "source": [
        "# Creating LSTM Model\n",
        "\n",
        "---\n",
        "\n",
        "- input_size: Corresponds to the number of features in the input. In this case, we use only the values so the input_size = 1. If we use 2 features: PC, memory load, then the input_size = 2.\n",
        "- hidden_layer_size: Specifies the number of hidden layers along with the number of neurons in each layer. The default is 100.\n",
        "- output_size: the number of items in the output, output_size = 1 is the predited valued. It is the predicted value.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-PzntAKz7Q2q"
      },
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_layer_size=100, output_size=1):\n",
        "        super().__init__()\n",
        "        self.hidden_layer_size = hidden_layer_size\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size, hidden_layer_size)\n",
        "\n",
        "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
        "\n",
        "        self.hidden_cell = (torch.zeros(1,1,self.hidden_layer_size),\n",
        "                            torch.zeros(1,1,self.hidden_layer_size))\n",
        "\n",
        "    def forward(self, input_seq):\n",
        "        lstm_out, self.hidden_cell = self.lstm(input_seq.view(len(input_seq) ,1, -1), self.hidden_cell)\n",
        "        predictions = self.linear(lstm_out.view(len(input_seq), -1))\n",
        "        return predictions[-1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "I8ZZYrM47tSr"
      },
      "source": [
        "model = LSTM()\n",
        "loss_function = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwhyH-LM75X8"
      },
      "source": [
        "# Training the Model\n",
        "\n",
        "---\n",
        "\n",
        "- The size of data is large, so if we increase the number of epochs, then it takes a long time to training the model with GPU.\n",
        "- In our experiment, the number of epochs varies from 10 to 60."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ohzv_xcUmkJB"
      },
      "source": [
        "epochs =  input(\"Please enter number of epochs: \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "062Icj7g73nN"
      },
      "source": [
        "#print(train_inout_seq)\n",
        "#for i in range(epochs):\n",
        "  \n",
        "for seq, labels in train_inout_seq:\n",
        "  optimizer.zero_grad()\n",
        "  model.hidden_cell = (torch.zeros(1, 1, model.hidden_layer_size),\n",
        "                        torch.zeros(1, 1, model.hidden_layer_size))\n",
        "\n",
        "  y_pred = model(seq)\n",
        "        #print(\"inside\")\n",
        "  single_loss = loss_function(y_pred, labels)\n",
        "  single_loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if i%25 == 1:\n",
        "      print(f'epoch: {i:3} loss: {single_loss.item():10.8f}')\n",
        "\n",
        "print(f'epoch: {i:3} loss: {single_loss.item():10.10f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94TaEZ8d8JN6"
      },
      "source": [
        "# Making Predictions\n",
        "\n",
        "---\n",
        "\n",
        "- We filtered 20% values from the data, and print out the output\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7FzmN5Di8Ood"
      },
      "source": [
        "fut_pred = (int)(0.2 * totalRows)\n",
        "\n",
        "test_inputs = train_data_normalized[-train_window:].tolist()\n",
        "print(test_inputs)\n",
        "print(len(test_inputs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Thm_dIOMXbu4"
      },
      "source": [
        "### Make Prediction\n",
        "\n",
        "---\n",
        "\n",
        "- The outer for loop will execute fut_pred times.\n",
        "- In the second iterator, the last tuples will be used as the input and a new prediction will be made which will then be appended to the test_inputs list again. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "57LwhpTI8Z1L"
      },
      "source": [
        "model.eval()\n",
        "\n",
        "for i in range(fut_pred):\n",
        "    seq = torch.FloatTensor(test_inputs[-train_window:])\n",
        "    with torch.no_grad():\n",
        "        model.hidden = (torch.zeros(1, 1, model.hidden_layer_size),\n",
        "                        torch.zeros(1, 1, model.hidden_layer_size))\n",
        "        test_inputs.append(model(seq).item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTIzFrb9YO1k"
      },
      "source": [
        "Because the training data is normalized, then the result is also normalized. We must transform the results back to the normal value using inverse_transform, and print the actual_prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "K2LSuE618f-C"
      },
      "source": [
        "test_inputs[fut_pred:]\n",
        "actual_predictions = scaler.inverse_transform(np.array(test_inputs[train_window:] ).reshape(-1, 1))\n",
        "print(actual_predictions)\n",
        "print(len(actual_predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0y0LEkXFCZeM"
      },
      "source": [
        "# Evaluate the results\n",
        "\n",
        "---\n",
        "\n",
        "- Actual value is last 20 percent of the total rows in all_data\n",
        "- Calculate the mean square errors\n",
        "- calculate square of difference between the predicted value and actual value\n",
        "- compare the Mean Square Error with the squared difference. \n",
        "  - If MSE < difference: do not choose the predicted value, Mark as 0\n",
        "  - if MSE >= difference: choose the predicted value, Mark as 1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1zcORzL4pykJ"
      },
      "source": [
        "print(len(test_data))\n",
        "print(len(actual_predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BEhlsWyZDUJS"
      },
      "source": [
        "# Actual prediction is the predicted value after transform\n",
        "# y_test = test_data\n",
        "#train_data = all_data[:-test_data_size]\n",
        "#test_data = all_data[-test_data_size:]\n",
        "\n",
        "#y_train_pred = scaler.inverse_transform(y_train_pred.detach().numpy())\n",
        "#y_train = scaler.inverse_transform(y_train.detach().numpy())\n",
        "#y_test_pred = scaler.inverse_transform(y_test_pred.detach().numpy())\n",
        "#y_test = scaler.inverse_transform(y_test.detach().numpy())\n",
        "\n",
        "# calculate root mean squared error\n",
        "#trainScore = math.sqrt(mean_squared_error(y_train[:,0], y_train_pred[:,0]))\n",
        "#print('Train Score: %.2f MSE' % (trainScore))\n",
        "#testScore = math.sqrt(mean_squared_error(y_test[:,0], y_test_pred[:,0]))\n",
        "#print('Test Score: %.2f MSE' % (testScore))\n",
        "import math\n",
        "from sklearn.metrics import mean_squared_error\n",
        "#trainScore = mean_squared_error(test_data, actual_predictions)\n",
        "#trainScore = math.sqrt(mean_squared_error(test_data, actual_predictions))\n",
        "#print('Train Score: %.2f MSE', (trainScore))\n",
        "threshold = (mean_squared_error(test_data, actual_predictions))\n",
        "print('Test Score: %.2f MSE' , (threshold))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlMemEn2Ikl6"
      },
      "source": [
        "# Print the PC, Memory Address, Actual Value(hex), Predicted Value, Decision\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "of1IqZyyFMDo"
      },
      "source": [
        "# Plot the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sQDNcM1ZFOe_"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "# Display the x value, PC and Memory address\n",
        "t = data.tail(len(test_data))\n",
        "decision = list()\n",
        "MSE = list()\n",
        "for i in range(len(test_data)):\n",
        "  if math.pow(test_data[i] - actual_predictions[i],2) > threshold:\n",
        "    MSE.append(math.pow(test_data[i] - actual_predictions[i],2))\n",
        "    decision.append('NO')\n",
        "  else:\n",
        "    MSE.append(math.pow(test_data[i] - actual_predictions[i],2))\n",
        "    decision.append('YES')\n",
        "\n",
        "#t['x'] = x\n",
        "#print(test_data)\n",
        "bases = np.amin(actual_predictions)\n",
        "#print(bases)\n",
        "t['Decision'] = decision\n",
        "#t['test_data'] = np.true_divide(test_data,bases)\n",
        "#t['actual_predictions'] = np.true_divide(actual_predictions ,bases)\n",
        "print(t)\n",
        "print(t.info())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfkCfkeKiVSC"
      },
      "source": [
        "# print number of yes (choose the value) and no (don't choose the value)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UYI5-381iCQl"
      },
      "source": [
        "print(t['Decision'].describe(include=['category']))\n",
        "print(\"Size: \", len(all_data))\n",
        "print(\"training size: \", len(train_data))\n",
        "print(\"test size: \", len(test_data))\n",
        "print(\"epochs: \", epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bz_Z3pQ4Tlcx"
      },
      "source": [
        "# Plot the threshold and mean square error for each load PC, and memory address\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cvFx55SUnm6"
      },
      "source": [
        "## Plot the test value, threshold and prediction value of each load PC and memory address"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Hqk8FZKTT-B7"
      },
      "source": [
        "plt.title('PC Address, Memory Address - Predicted value and actual value')\n",
        "plt.ylabel('Value')\n",
        "plt.grid(True)\n",
        "plt.autoscale(axis='PC Address, Memory Address', tight=True)\n",
        "x_length = len(test_data)\n",
        "x = list(range(0,x_length))\n",
        "#plt.plot(x,test_data, color=\"red\")\n",
        "#plt.plot(x,actual_predictions,color=\"blue\")\n",
        "plt.plot(x,np.true_divide(test_data,bases), color=\"red\")\n",
        "plt.plot(x,np.true_divide(actual_predictions,bases),color=\"blue\")\n",
        "#plt.plot(x,np.array([threshold for i in range(len(test_data))]),color=\"Green\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBKvzk12UziL"
      },
      "source": [
        "## plot the threshold and Squared difference of actual value and predicted value of each load PC and memory address."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQar_tSQ7E_4"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1pgvg4JjUEg-"
      },
      "source": [
        "plt.title('PC Address, Memory Address - Predicted value and actual value')\n",
        "plt.ylabel('Value')\n",
        "plt.grid(True)\n",
        "plt.autoscale(axis='PC Address, Memory Address', tight=True)\n",
        "x_length = len(test_data)\n",
        "x = list(range(0,x_length))\n",
        "plt.plot(x,MSE, color=\"red\")\n",
        "plt.plot(x,np.array([threshold for i in range(len(test_data))]),color=\"Green\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}